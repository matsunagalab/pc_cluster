# PCクラスタ管理メモ

---

## Slurm

Slurmの設定に関しては、日本語情報では以下が参考になる

- http://www-fps.nifs.ac.jp/ito/memo/slurm01.html

- https://biomodeling.co.jp/2019/11/06/slurmmpichcudaの設定centos7/

### mungeのインストール

mungeは認証に使われる。
本家の[ドキュメント](https://github.com/dun/munge/wiki/Installation-Guide)
がよくまとまっているので、主にこれに従う。
ubuntuの場合はパッケージでインストールできる。
パッケージをインストールすると `munge` というユーザが自動的に作られる。

```bash
sudo apt update
sudo apt install -y munge libmunge-dev libmunge2
```

鍵ファイル `munge.key` を作成して共有。鍵は以下に置く。
```bash
/etc/munge/munge.key
```
`munge.key` のパーミッションは `0600` でなければならない。

### slurmのインストール

コンパイルしてインストール。本家の[ドキュメント](https://slurm.schedmd.com/quickstart_admin.html)
は非常にわかりにくいが細かな情報はこれを読むしかない。
defaultでは `/usr/local` 以下へインストールされる。
Contorlのための `slumctld` と clientのための `slurmd` デーモンが作成される(他のもあるが割愛)。
```bash
./configure --with-hdf5=no
make
sudo make install
```

`slurm.conf` を作成して以下へ置く。全てのノードで同じ内容のファイルを置く。
```
/usr/local/etc/slurm.conf
```

`slurm.conf` の中身は以下。コア数などは `lscpu` や `sudo slurmd -C` などで確認しながら記入する。

```bash
# slurm.conf file generated by configurator easy.html.
# Put this file on all nodes of your cluster.
# See the slurm.conf man page for more information.
#
SlurmctldHost=crab
# 
#MailProg=/bin/mail 
MpiDefault=none
#MpiParams=ports=#-# 
ProctrackType=proctrack/pgid
ReturnToService=1
SlurmctldPidFile=/var/run/slurmctld.pid
#SlurmctldPort=6817 
SlurmdPidFile=/var/run/slurmd.pid
#SlurmdPort=6818 
SlurmdSpoolDir=/var/spool/slurmd
#SlurmUser=slurm
SlurmdUser=root 
StateSaveLocation=/var/spool
SwitchType=switch/none
TaskPlugin=task/affinity
# 
# 
# TIMERS 
#KillWait=30 
#MinJobAge=300 
#SlurmctldTimeout=120 
#SlurmdTimeout=300 
# 
# 
# SCHEDULING 
SchedulerType=sched/backfill
SelectType=select/cons_tres
SelectTypeParameters=CR_Core
# 
# 
# LOGGING AND ACCOUNTING 
AccountingStorageType=accounting_storage/none
ClusterName=cluster
#JobAcctGatherFrequency=30 
JobAcctGatherType=jobacct_gather/none
#SlurmctldDebug=info 
#SlurmctldLogFile=
#SlurmdDebug=info 
#SlurmdLogFile=
# 
# 
# COMPUTE NODES 
GresTypes=gpu

NodeName=crab CPUs=64 RealMemory=512000 Sockets=2 CoresPerSocket=16 ThreadsPerCore=2 State=UNKNOWN
NodeName=n[1-2] Gres=gpu:titan:10 CPUs=64 RealMemory=64000 Sockets=2 CoresPerSocket=16 ThreadsPerCore=2 State=UNKNOWN
NodeName=n[3-4] Gres=gpu:1080:10 CPUs=64 RealMemory=64000 Sockets=2 CoresPerSocket=16 ThreadsPerCore=2 State=UNKNOWN
NodeName=n5 Gres=gpu:2080:10 CPUs=32 RealMemory=64000 Sockets=2 CoresPerSocket=16 ThreadsPerCore=1 State=UNKNOWN
NodeName=m1 Gres=gpu:8000:2 CPUs=28 RealMemory=512000 Sockets=1 CoresPerSocket=28 ThreadsPerCore=1 State=UNKNOWN

PartitionName=crab Nodes=crab Default=NO MaxTime=INFINITE State=UP
#PartitionName=no_n1 Nodes=n[2-5] Default=NO MaxTime=INFINITE State=UP
PartitionName=all Nodes=crab,n[1-5],m1 Default=YES MaxTime=INFINITE State=UP
```

Generic ResoucesとしてGPUを管理下に入れている。
GPUを搭載するCompute nodeでは、GPU を自動認識してもらうために同じ場所に `gres.conf` を作成。
```bash
sudo echo "AutoDetect=nvml" >/usr/local/etc/gres.conf
```

`slurm.conf`の中で指定しているgpuの名前(例えば `Gres=gpu:titan:10`)を知るには、
本家の[ドキュメント](https://slurm.schedmd.com/gres.html)によると、
以下のようにすれば良いらしい。
```
To see the GPU name, set SlurmdDebug=debug2 in your slurm.conf and either restart or reconfigure slurmd and check the slurmd log. For example, with AutoDetect=nvml:
```

しかしgpuの名前を知るには、`slurm.conf`でとりあえず名前は指定せずにしておいて、
`sudo slurmd -D` でデーモンを直接起動してnvmlからの出力を見るほうが楽である。

その後、firewallを適用している場合にはslurm通信用のポートを解放する。
```bash
sudo firewall-cmd --zone=external --add-port=6817/tcp --permanent
sudo firewall-cmd --zone=external --add-port=6817/udp --permanent
sudo firewall-cmd --zone=external --add-port=6818/tcp --permanent
sudo firewall-cmd --zone=external --add-port=6818/udp --permanent
sudo firewall-cmd --reload
```

`slurm.conf` の設定が上手く行っているかどうかを
`sudo slurmd -D` として対話的にデーモンを実行しながら確認。

エラーがなくなって、正常に起動するようになったら
Controller node で `slurmctld` デーモンを動かす
compute node で `slurmd` デーモンを動かす

`sinfo` で status が `drain` となってしまう場合は、
`scontrol show node ホスト名` などで理由をみる。

動作するようになったらsystemdへ登録する

```bash
sudo cp etc/slurmctld.service /etc/systemd/system
sudo cp etc/slurmd.service /etc/systemd/system

# On Controller node
sudo systemctl start slurmctld
sudo systemctl status slurmctld
sudo systemctl enable slurmctld

# On Compute node
sudo systemctl start slurmd
sudo systemctl status slurmd
sudo systemctl enable slurmd
```

---

## NIS

新しめのUbuntuの場合は、以下でインストールできる。
インストール時にNISドメイン名を聞かれるので入力する。

```
$ sudo apt -y install nis
```

### ホームディレクトリを空にする

NISとNFSでホームディレクトリを共有する場合は、
ローカルユーザのディレクトリがあってバッティングするとややこしいので
ローカルユーザのディレクトリは別の場所へ移す。例えば `home_local` など。

```
sudo usermod -d /home_local/user_name user_name
```

### client

`/etc/yp.conf` にNISドメイン名とサーバ名(orIPアドレス)を記入

```
domain NISドメイン名 server サーバ名
```

更に、`/etc/nsswitch.conf` の中のNISで共有したい情報に `nis` を追加


```
passwd:         files systemd nis
group:          files systemd nis
shadow:         files nis
gshadow:        files

hosts:          files mdns4_minimal [NOTFOUND=return] dns
networks:       files

protocols:      db files
services:       db files
ethers:         db files
rpc:            db files

netgroup:       nis
```

systemdでサービス起動。
```
systemctl restart rpcbind nis 
systemctl status nis
```

`ypwhich` でNISサーバまたはマップマスタの名前が返るはず。

```
ypwhich
```

`ypcat` でマップされた情報を確認できる。例えば

```
ypcat passwd
```

問題なければ登録する。

```
systemctl enable rpcbind nis 
```

### server

`/etc/sysconfig/network` にNISドメイン名と、必要があればNISで用いるポート番号を登録。

```
NISDOMAIN=NISドメイン名
YPSERV_ARGS="-p ポート番号"
```

`authconfig` でNISサーバ情報を登録する。

必要に応じて、設定したポート番号を `firewall-cmd` で解放。

サービスの登録。

---

